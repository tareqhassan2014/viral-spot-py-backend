#!/usr/bin/env python3
"""
Simple crawler startup script - self-contained version
"""
import asyncio
import sys
import os

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("Warning: python-dotenv not installed. Install with: pip install python-dotenv")
    print("Or set environment variables manually")

async def start_discovery():
    """Start simple profile discovery - get similar profiles and queue 4 best ones"""
    
    # Import the enhanced crawler with Supabase support
    from network_crawler import create_crawler
    
    print("ğŸš€ Starting Instagram Network Discovery...")
    print("ğŸ¯ Simple discovery: find similar profiles and queue 4 best ones")
    
    # Check if Supabase is available
    try:
        from supabase_integration import SupabaseManager
        supabase_available = True
        print("â˜ï¸ Supabase integration: AVAILABLE")
    except:
        supabase_available = False
        print("ğŸ’¾ Storage: CSV only (Supabase not configured)")
    
    # Create crawler with simple config
    crawler = await create_crawler()
    
    # Get the starting profile (resume or default)
    start_profile = crawler.get_resume_profile()
    
    print(f"ğŸ“ Starting from: @{start_profile}")
    print(f"ğŸ¯ Will queue max 4 accounts for scraping")
    
    # Check RapidAPI key
    api_key = os.getenv('RAPIDAPI_KEY') or os.getenv('INSTAGRAM_SCRAPER_API_KEY')
    if not api_key:
        print("âš ï¸ Warning: No RapidAPI key found")
        print("   Set RAPIDAPI_KEY or INSTAGRAM_SCRAPER_API_KEY environment variable")
        return
    
    # Start discovery
    try:
        result = await crawler.start_discovery([start_profile])
        print("\nâœ… Discovery completed!")
        print(f"ğŸ” Similar found: {result.similar_found} profiles")
        print(f"ğŸ¯ Selected: {result.selected} profiles")
        print(f"ğŸ“‹ Queued for scraping: {result.queued} accounts")
        print(f"â­ï¸ Duplicates skipped: {result.skipped_duplicates}")
        
        if result.queued > 0:
            print(f"\nğŸ‰ Success! {result.queued} profiles added to queue")
            if supabase_available and crawler.use_supabase:
                if crawler.supabase and crawler.supabase.keep_local_csv:
                    print("â˜ï¸ Profiles saved to both Supabase and CSV")
                else:
                    print("â˜ï¸ Profiles saved to Supabase")
            else:
                print("ğŸ’¾ Profiles saved to local queue")
            print("\nğŸ“‹ Start the queue processor to begin scraping:")
            print("   python queue_processor.py --start")
        
    except Exception as e:
        print(f"âŒ Discovery failed: {e}")

if __name__ == "__main__":
    asyncio.run(start_discovery())